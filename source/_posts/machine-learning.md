---

title: 一场GD和SGD的实验与较量
date: 2016-12-27 21:13:44
tag: 
  - Machine Learning  
categories: Technology
clearReading: true
thumbnailImage: http://photocdn.sohu.com/20151112/mp41310884_1447301437549_6.jpeg
thumbnailImagePosition: right
autoThumbnailImage: yes
metaAlignment: center
comments: true

---

为了比赛赶了一篇没什么质量的机器学习论文，也算开了一个巨坑吧
主要就是吐槽机器学习梯度下降算法啦～

<!-- more -->
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器学习是近几年兴起的一种人工智能分支，关于人工智能的大多数研究表明：人工智能是从以“推理”为重点再拓展到以“学习”为重点的一条结构清晰的知识脉络，而机器学习就是人工智能这条脉络的主干，或者说是解决人工智能的一条关键途径。关于机器学习，我们认为其本质就是一种让机器不断进行自身优化从而达到智能化、网络化进而取代人工劳动力、改善生活的一种途径。在近30年的发展历程中，机器学习已经由一项起源于计算机、关联于硬件编程的技术发展成为了一门多领域交叉学科，其中涉及统计学、概率论、逼近论、计算复杂性理论等。研究机器学习，不仅仅可以加深互联网时代人工智能化的普及，还可以对现代许多科技领域（如：数据挖掘、计算机视觉、生物特征识别、医学诊断）的发展有着深远的影响。本文主要分为三部分，第一部分对已有的机器学习原理和概念进行介绍和综述。第二部分对一些普遍的机器学习优化算法进行实例详解和分析（线性回归模型）。第三部分提出对已有的机器学习算法（梯度下降算法）进行的改进和新旧算法它们的优劣对比。

### 研究框架以及机器学习概念综述

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年Google IO大会上，Google 公司宣布将自己设计研发的深度学习框架TensorFlow彻底开源，本文的所有算法实验都是基于TensorFlow的Python API完成的，但是算法是所有机器学习理论通用的，不会受限于编程语言。关于机器学习，上文已经只出了作者自己的定义，现在较为公认的说法是建立在卡内基梅陇大学的米歇尔教授对机器学习的定义之上的，在此引用维基百科的相关定义：

> “机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。
> 机器学习是对能通过经验自动改进的计算机算法的研究。
> 机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。
> 一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中在机器学习之下又细分为深度学习，这是一种独特的机器学习算法，适用于搭建深度神经网络来解决技术上的问题。撇开深度学习的算法不说，机器学习的学习方式主要又可以分为这么几类：

(1)	*监督学习*：在给定的训练数据集中学习出一个函数，当有新的数据输入时，监督学习可以根据这个函数预测出结果。其中训练集的目标是由人为标注的。这种学习方式在生活中具体应用广泛，可以是在防垃圾邮件系统中区分“垃圾邮件”和“非垃圾邮件”的分类，也可以是数字手写识别中的数字区分等。在不断地比对“预测结果”和“训练数据”之中，监督式学习建立的学习过程将不断地调整预测模型，最终使得模型的预测结果达到一个预期的准确率。

(2)	*非监督学习*：和监督学习对比，无监督学习输入的训练集没有人为标注的结果，学习模型是为了推断出数据的一些内在结构，类似于面向对象编程的抽象，非监督学习会将输入数据的共同点进行抽象，最终整合出一个学习目标。它的应用场景包括关联规则的学习以及聚类等，通常我们会使用包括Apriori以及k-Means在内的算法来实现。

(3)	*半监督学习*：顾名思义，这是一种介于监督学习和半监督学习之间的学习方式，其中输入数据集部分被标识，部分没有标识。这种学习模型可以用来预测，但是预测的前提就是学习数据的内在结构，这样才能合理地组织数据。具体的应用场合包括分类和回归。算法主要包括一些对常用监督学习的算法延伸（如：GraphInference或Laplacian SVM等）。

(4)	*增强学习*：这和之前三种习方式不同，增强学习通过观察反馈来完成学习目标的判断。在这种学习模式下，输入数据直接反馈到模型，模型必须对此立刻做出调整。常见的应用包括动态系统以及机器人控制等。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总结上述几种机器学习方式，我们发现使得计算机完成训练模型的功能最重要的就是输入训练数据集，其中可以在输入训练数据集中设定静态学习目标或是由计算机自己判断、诱导、观察反馈等实现特定的目标，只有确定了明确的学习目标，计算机在进行算法迭代和训练人工智能模型的时候才可以有效的完成模式识别。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据输入数据类型的不同，对于一个问题的建模有着不同的方式。在机器学习的领域，我们首先会考虑的就是算法的学习方式。我们将会在文章后半部分详细介绍机器学习的算法，以及对常见的一些算法提出替换和优化的策略。

### 机器学习模型实践以及分析
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中，训练出准确的模型是一切算法的最终目的。在具体介绍算法之前，我们先对数学中最常见的一元线性回归问题建立一个基于Google TensorFlow的机器学习模型。在统计学中，线性回归是利用线性回归方程的最小二乘法对一个或者是多个自变量和因变量之间的关系进行建模的一种回归分析。在线性回归中，数据使用线性预测来构建模型，而且在绘制回归模型的时候出现的未知参数也是由数据来估计的，所以虽然这是个很简单的问题，我们却可以使用它来驮载机器学习的算法。
我们假设了存在这么一个线性模型（类似线性回归模型）：

![tensorflow3](https://huangyz0918.github.io/assets/images/famular1.png)

其中θ是参数，然后我们构建一个成本函数：

![tensorflow3](https://huangyz0918.github.io/assets/images/famular2.png)
 
那么每次梯度下降算法的更新算法就是：

![tensorflow3](https://huangyz0918.github.io/assets/images/famular3.png)
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在编程代码中，我们先对线性回归定义一组初始值，计算机不依赖于线性回归方程，只是根据初始值来构建回归模型，在不断地自我修正中达到对回归曲线的精准预测。我们实验的初始值如下：

``` python

"""
tensorflow训练的初始数据
"""

trX = np.random.normal(0.0,0.5)

trY = 0.22*x+0.78+np.random.normal(0.0,0.03)
```

这是一组XY坐标点几乎均匀分布在一次函数  附近的初始数据点，为了达到最好的模型准确度调试结果，我们设定TensorFlow预测的曲线与正比例函数数据点分布相似度越高，则准确度越大。在Python代码中，我们创建了一个机器学习框架会话(Session)并且将我们训练模型的操作通过这个会话执行。另外，我们设定一个固定的机器学习速率，以这个学习速率为基础，只改变迭代次数x进行实验：
``` python
#定义学习率：
learning_rate = 0.50

#对模型训练x次：
for i in xrange(x):
    for (x, y) in zip(trX, trY):
        sess.run(train_op, feed_dict={X: x, Y: y})
        
```

当我们训练次数为x = 0 时得到如下输出结果：

 ![tensorflow3](https://huangyz0918.github.io/assets/images/tensorunfit.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很明显，机器学习算法预测的绿色直线并没有很准确的落在线性回归初始数据上面，这个预测是不准确的，因为根据这样的一个学习速率，计算代码是无法准确地完成对所有数据点的抽象和分类的。但是一旦我们修改机器学习的迭代学习次数x使得它达到合理的大值，这时得到的机器预测直线就会无限趋近于正确的函数分布，我们令迭代学习次数x = 20，实验结果如下图所示：

 ![tensorflow3](https://huangyz0918.github.io/assets/images/tensormatch.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时可以看到，我们的机器学习算法在很大成都上实现了对于线性回归模型的预测，这也就是机器学习算法的优势和魅力所在。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们这个实验基于的是Google 开源框架 TensorFlow，其实实现这个一阶线性回归模型过程的思路并不复杂，不论是在TensorFlow还是任何其他的机器学习框架之下，这套算法的核心思路都是不变的，其本质就是找出一条线，这条线能够很好的拟合平面上的这些数据点，而寻找这条线就要使得数据点到达直线的平方距离最短。于是我们定义了一个计算平方距离的成本函数（Cost function），通过梯度下降的不断迭代来不断下降这个成本函数的值，直到成本函数无限地变小，当成本函数有一个可行的最小值时完成模型的构建。现在在机器学习领域使用得最广的最小化成本函数算法就属梯度下降算法了。在文章的下一节里面，我们将介绍关于梯度下降算法的基本概念并且分析梯度下降算法的缺点，最终针对它的一些缺点提出一个可行的优化方案，并且进行实验证明。

### 梯度下降算法优劣分析以及优化方案

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所谓“梯度下降算法”实际上只是一种最基础的优化算法，是一个一阶算法，通常也被称为最速下降法。在机器学习里面被广泛的使用，本文上述部分就是采用的梯度下降算法来达到实现训练线性回归模型的。但是梯度下降算法也有不能避免的缺点：

> (1)	遍历训练数据时越到函数极值点，梯度下降算法计算模型所需要的时间就越长。


> (2)	梯度下降法不适用于初始值数量巨大的数据，在计算大量数据的时候，梯度下降算法所耗费的时间以极高的速率增长

> (3)	当机器学习定义的学习速率太高时，梯度下降算法会在遍历初始训练数据的时候跳过极值点，使得回归模型预测线上下浮动。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了研究梯度下降算法的缺点，我们做了如下实验：保持其他参数不变，仅仅改变学习率，使学习率增高，观察计算代码输出的log(包括根据初始值分配的权重a和偏差b)和Python代码绘制的图表。我们知道，当算法学习率为0.5的时候，输出图像是一切正常的。然后我们将学习率改为1.0，这个时候会发现输出有了明显的变化。从图表上可以直观的看到，随着迭代的次数不断增加，预测直线在真实值上下剧烈跳动：

 ![tensorflow3](https://huangyz0918.github.io/assets/images/tensorfour.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;仅仅是在迭代次数为 0、5、10、15的训练次数下，机器预测曲线就上下摆动了多次，这样的预测效果可以很明显判断为失真。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们还针对上述实验，从偏差和权重的角度分析了梯度下降算法在高学习率下的缺点，得到了如下的实验数据：（其中图1是在学习率为0.5条件下经过0、5、10、15次迭代得到的权重和偏差数据；图2是在学习率为1.0条件下经过0、5、10、15次迭代得到的权重和偏差数据）

![tensorflow3](https://huangyz0918.github.io/assets/images/testdata1.png)
                  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可见依靠梯度下降算法来实现高学习率条件下的模型构建是不科学的，这样不仅仅会得到失真的预测结果，应对这个问题的方式一般只有调小学习率，虽然会增加迭代次数，但是可以得到一个可靠的模型。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度下降还有一个很大的问题，通过这样的一个迭代算法我们可以看出：在对成本函数求偏导数的时候，是需要用到全部的训练数据的，正是因为如此，当输入的初始数据非常庞大的时候，传统的梯度下降算法就会花费巨大的时间以及计算机内部的运算资源。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;针对上述关于梯度下降算法的缺点，我们找出了一个可行的优化策略：使用随机梯度下降算法对已有的梯度下降算法进行优化甚至替代。那么，什么是随机梯度下降算法呢？
	
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度下降算法(GD)和随机梯度下降算法(SGD)从名字上听其实差别不大，但是为什么随机梯度下降算法就能够有效的优化之前梯度下降算法的缺点呢？在说不同点之前，我们先研究了它们的相同之处，即无论在梯度下降算法还是随机梯度下降算法中，每次都会在迭代中更新模型的参数，使得成本函数变小，所以可以说它们使成本函数变小的方式是一样的。关于不同之处，在以往的梯度下降算法中，每次迭代都要使用到全部的训练数据。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而对于随机梯度下降算法来说，同样是一个线性模型，每次迭代可以只使用一个随机的数据来更新参数。我们假设现在对于这个线性模型，输入的数据集只有一条(x,y):

![tensorflow3](https://huangyz0918.github.io/assets/images/famular4.png)

这样来看的话，我们更新参数的算法就会变成：

![tensorflow3](https://huangyz0918.github.io/assets/images/famular5.png)
           
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此时更新的算法，只用到了一个样本数据，这样分析下来，使用随机梯度下降算法的话，在计算机读取到一条新的数据的时候，根据这条数据计算出来的值和实际值的差距就会成为决定参数更新的幅度大小，也就是说，使用随机梯度下降算法可以有效的减小计算成本函数的时间。
为此我们做了对比实验，实验中，我们先使用梯度下降算法对线性模型进行了迭代和训练，之后使用改进了的算法进行同样模型的运算。可以看到，在完成180次迭代之后，两种算法的时间差大概是0.003s，随机梯度下降算法的运行时间大约是传统梯度下降算法运行时间的二分之一。

![tensorflow3](https://huangyz0918.github.io/assets/images/testdata2.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图4

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上面的数据我们可以勉强看到梯度下降算法和随即梯度下降算法上时间的区别。当然，由于我们使用的是上述简单的线性回归模型，所以两组实验的区别不大，但是正是这样看似微乎其微的差别，在大数据面前梯度下降算法所耗费的时间将会爆炸式增长。所以将梯度下降算法进行优化，是机器学习领域的一个重要的算法优化环节。如果能够合理的应用改进以后的随即梯度下降算法，取到合适的机器学习速率x，可以大大提高预精准测值出现的效率，从这一点上面来说，改进了的随机梯度下降算法对无论是机器学习还是数学分析都有着极为重要的作用。




<!-- more -->